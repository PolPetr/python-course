# -*- coding: utf-8 -*-
"""Копия блокнота "3_cleaning.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pnrvKeUEvNf3VwajSdFSD9IlIvob8DSf

**Data Cleaning** (чистка данных) - этап предварительной обработки данных для анализа данных и машинного обучения.

**Примеры Data Cleaning:**
- удаление избыточных столбцов из табличных данных;
- приведение текста к нижнему регистру;
- чистка текста от HTML-артефактов

Загрузим тренировочный датасет, почистим его и проанализируем результаты.

# Задание 1: Загрузка данных

Загрузим тренировочный датасет и посмотрим на наши данные.

Представим, что мы загрузили статью в формате HTML.

Скачаем ее и выведем на экран первые 100 символов с помощью слайсинга (среза).
"""

# загружаем тренировочные данные
!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/data_cleaning.txt

# запишем данные в переменную data
with open('data_cleaning.txt', 'r') as f:
  data = f.read()

# выведите на экран первые 100 символов с помощью слайсинга

print(data[:100])

"""# Задание 2: Удаление артефактов

В данных много артефактов - HTML-тегов.

Удалим HTML-артефакты с помощью регулярных выражений RegEx
"""

# пропишем паттерн для поиска HTML-тегов вида <tag> ... </tag>
import re   # загрузим библиотеку для обработки регулярных выражений

tag_pattern = r'<[^>]+>'    # паттерн для поиска тегов

"""Используйте функцию `re.sub` (substitution) для чистки данных

`re.sub` ищет в строке `string` соответствия RegEx-паттерну `pattern` и меняет найденное на указанную строку `repl`

Как используем функцию: `re.sub(pattern, repl, string)`

- `pattern` - паттерн RegEx, соответствия которому будет искать функция
- `repl` - на что будем менять найденные соответствия
- `string` - где будем искать, наш датасет

Запишите результат в переменную `clean_text` и выведите на экран с 720-го по 800-ый символ очищенного текста

Используйте слайсинг
"""

# Подсказки:
# используйте паттерн, записанный в переменную tag_pattern
# замените результат на пустую строку ""
clean_text = re.sub(tag_pattern, "", data)
print(clean_text[720:800])

"""Мы удалили не все специальные символы HTML

Создадим еще один паттерн и повторим процедуру
"""

symbols_pattern = r'&\w+;'    # паттерн для поиска специальных символов

"""Используйте `re.sub` для удаления этих символов

Теперь функция принимает на вход паттерн `symbols_pattern` и текст `clean_text`

Перезапишите переменную `clean_text`

Выведите на экран с 720-го по 800-ый символ, чтобы убедиться в том, что чистка прошла успешно
"""

clean_text = re.sub(symbols_pattern, "", clean_text)
print(clean_text[720:800])

"""В нашем тексте остались двойные пробелы

Уберем им с помощью очередного паттерна
"""

# выведите на экран первые 100 символов вашего текущего результата
# на этот раз не используйте print
clean_text = re.sub(r' {2,}', ' ', clean_text[:100]).strip()
clean_text[:100]

"""Создаем паттерн для поиска двойных пробелов

Повторите процедуру, перезапишите результат в `clean_text` и выведите первые 100 символов

Что мы запишем в переменную `repl`, чтобы не удалить абсолютно все пробелы?
"""

space_pattern = ' {2,}'
repl = ' '
clean_text = re.sub(space_pattern, repl, clean_text)
clean_text[:100]
### ваш код здесь: примените re.sub ###
### ваш код здесь: выведите первые 100 символов, не используя print ###

#с этого момента все пошло не так, судя по выдаче. я пока не понимаю, в чем проблема

"""# Задание 3: Смена регистра

Приведем все слова к нижнему регистру с помощью функции `lower`

Запишем результат в переменную `text_lower` и выведем на экран последние 100 символов
"""

### ваш код здесь: примените lower к clean_text ###
### ваш код здесь: выведите первые 100 символов с конца, используйте слайсинг и не забудьте про - ##
text_lower = clean_text.lower()
text_lower[:-100]

"""# Задание 4: Удаление стоп-слов

Удалим частотные слова, которые создают шум для решения задач

Загрузим список стоп-слов
"""

!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt

# запишем данные в переменную stopwords
with open('stopwords.txt', 'r') as f:
  stopwords = f.read().split()

"""Выведите на экран первые 10 стоп-слов"""

stopwords[:10]

"""С помощью `random` мы можем "вытянуть" из списка стоп-слов случайное слово"""

import random
random.choice(stopwords)

""""Вытяните" еще одно случайное слово и запишите его в переменную `random_word`"""

random_word = random.choice(stopwords)
print(random_word)

"""Проверьте, есть ли это слово в `text_lower` с помощью `in`

Выведите на экран это слово
"""

### ваш код здесь: вывод на экран текста "Результат проверки:" и проверки с in ###
### ваш код здесь: вывод текста "Случайное слово:" и random_word ###
if random_word in clean_text:
  print(f"Результат проверки: '{random_word}'")
  print(f"Случайное слово: '{random_word}'")

"""Попробуйте сгенерировать еще несколько слов и проверить их наличие в `text_lower`

Для этого запустите повторно две последние ячейки

Вот так будет выглядеть текст после удаления стоп-слов _без_ токенизации
Заменятся все аналогичные сочетания знаков
Поэтому перед _удалением_ стоп-слов, проведем токенизацию

# Задание 5: Токенизация

Токенизируем датасет для дальнейшей работы

Создадим 2 набора токенов: с сегментацией по предложениям и с сегментацией по словам

Создайте переменную `sentences`

С помощью `split` разделите текст на предложения (сегменты, разделенные знаком `.`)

Выведите на экран первые 10 элементов `sentences`
"""

### ваш код здесь: split для сегментации по знаку `.` ###
### вывод на экран первых 10 предложений ###
sentences = text_lower.split('.')
sentences[:10]
#мне кажется, где-то выше ошибка в коде, потому что выводятся html и прочий мусор...

"""Создайте переменную `tokens`

С помощью `split` разделите текст `text_lower` на слова

Выведите первые 10 элементов
"""

### ваш код здесь: split для сегментации по пробелу ###
### ваш код здесь: вывод на экран первых 10 слов ###
tokens = text_lower.split()
tokens[:10]

""" Удалим стоп-слова"""

clean_tokens = []

for token in tokens:  # итерация по списку токенов tokens
  if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов
    clean_tokens.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах

clean_tokens[:10]

"""# И еще одно задание...

В ячейке ниже вы сможете загрузить еще один текст

Используйте свой код и код из задания, чтобы

1. удалить артефакты (html-теги, специальные символы и двойные пробелы)

2. привести текст к нижнему регистру

3. токенизировать текст по предложениям

4. токенизировать текст по словам

5. удалить стоп-слова
"""

!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/extracurricular/artefacts.txt

# запишем данные в переменную artefacts
with open('artefacts.txt', 'r') as f:
  artefacts = f.read()

cleaning_text = re.sub(r'<[^>]+>', '', artefacts)
#print(cleaning_text)

cleaning_text = re.sub(r'&\w+;', '', cleaning_text)
#print(cleaning_text)

clean_text = re.sub(r' {2,}', ' ', cleaning_text)
#print(cleaning_text)

new_text = cleaning_text.lower()
#print(new_text)

sentences = new_text.split('.')
#print(sentences)

tokens = new_text.split()
#print(tokens)

clean_tokens = []

for token in tokens:  # итерация по списку токенов tokens
  if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов
    clean_tokens.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах

clean_tokens[:10]